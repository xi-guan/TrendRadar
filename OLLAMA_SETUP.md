# Ollama æœ¬åœ°æ¨¡åž‹é…ç½®æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©æ‚¨ä½¿ç”¨ Ollama è¿è¡Œ TrendRadar çš„æœ¬åœ°å¤§æ¨¡åž‹ï¼Œå®žçŽ°å®Œå…¨ç¦»çº¿ã€å…è´¹çš„ AI åŠŸèƒ½ã€‚

## ä¸ºä»€ä¹ˆé€‰æ‹© Ollamaï¼Ÿ

- âœ… **å®Œå…¨å…è´¹**ï¼šæ— éœ€ API Keyï¼Œæ— ä½¿ç”¨é™åˆ¶
- âœ… **éšç§ä¿æŠ¤**ï¼šæ•°æ®å®Œå…¨æœ¬åœ°å¤„ç†ï¼Œä¸ä¸Šä¼ äº‘ç«¯
- âœ… **ä¸­æ–‡ä¼˜åŒ–**ï¼šæ”¯æŒ Qwenã€ChatGLM ç­‰ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–çš„æ¨¡åž‹
- âœ… **ç®€å•æ˜“ç”¨**ï¼šä¸€é”®å®‰è£…ï¼Œå‘½ä»¤è¡Œç®¡ç†æ¨¡åž‹
- âœ… **æ€§èƒ½ä¼˜ç§€**ï¼šæ”¯æŒ GPU åŠ é€Ÿï¼ŒæŽ¨ç†é€Ÿåº¦å¿«

## 1. å®‰è£… Ollama

### macOS / Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows
ä¸‹è½½å®‰è£…åŒ…ï¼šhttps://ollama.com/download/windows

### éªŒè¯å®‰è£…
```bash
ollama --version
```

## 2. ä¸‹è½½ä¸­æ–‡æ¨¡åž‹

### æŽ¨èæ¨¡åž‹ï¼ˆæŒ‰ä¸­æ–‡èƒ½åŠ›æŽ’åºï¼‰

| æ¨¡åž‹ | å‘½ä»¤ | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | ä¸­æ–‡èƒ½åŠ› | æŽ¨èç”¨é€” |
|------|------|--------|----------|----------|----------|
| **Qwen2.5** | `ollama pull qwen2.5:14b` | 14B | 16GB | â­â­â­â­â­ | ç»¼åˆæœ€ä½³ï¼Œå¼ºçƒˆæŽ¨è |
| Qwen2.5 (å°) | `ollama pull qwen2.5:7b` | 7B | 8GB | â­â­â­â­ | ä½Žé…ç½®æœºå™¨ |
| ChatGLM3 | `ollama pull chatglm3:6b` | 6B | 8GB | â­â­â­â­ | å¯¹è¯å‹å¥½ |
| DeepSeek-V2 | `ollama pull deepseek-v2:16b` | 16B | 20GB | â­â­â­â­â­ | æŽ¨ç†èƒ½åŠ›å¼º |

### ä¸‹è½½ Embeddings æ¨¡åž‹ï¼ˆç”¨äºŽå‘é‡æ£€ç´¢ï¼‰
```bash
ollama pull nomic-embed-text
```

## 3. é…ç½® TrendRadar

### æ–¹æ³• 1ï¼šçŽ¯å¢ƒå˜é‡ï¼ˆæŽ¨èï¼‰

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# LLM é…ç½®
LANGCHAIN_PROVIDER=ollama
LANGCHAIN_MODEL=qwen2.5:14b
LANGCHAIN_BASE_URL=http://localhost:11434  # Ollama é»˜è®¤åœ°å€

# Embeddings é…ç½®ï¼ˆç”¨äºŽå‘é‡æ£€ç´¢ï¼‰
LANGCHAIN_EMBEDDINGS_PROVIDER=ollama
LANGCHAIN_EMBEDDINGS_MODEL=nomic-embed-text
LANGCHAIN_EMBEDDINGS_BASE_URL=http://localhost:11434

# å…¶ä»–é…ç½®ï¼ˆå¯é€‰ï¼‰
LANGCHAIN_TEMPERATURE=0.3
LANGCHAIN_MAX_TOKENS=2000
LANGCHAIN_TIMEOUT=120
```

### æ–¹æ³• 2ï¼šå‘½ä»¤è¡Œå¯¼å‡º
```bash
export LANGCHAIN_PROVIDER=ollama
export LANGCHAIN_MODEL=qwen2.5:14b
export LANGCHAIN_EMBEDDINGS_PROVIDER=ollama
export LANGCHAIN_EMBEDDINGS_MODEL=nomic-embed-text
```

## 4. è¿è¡Œæµ‹è¯•

```bash
# å®‰è£…ä¾èµ–
uv sync --group langchain

# è¿è¡Œæµ‹è¯•
uv run python test_langchain_integration.py
```

## 5. æ¨¡åž‹æŽ¨è

### æ–°é—»æ‘˜è¦å’Œåˆ†æžï¼ˆæŽ¨è Qwen2.5ï¼‰
```bash
# é«˜æ€§èƒ½ï¼ˆéœ€è¦ 16GB+ å†…å­˜ï¼‰
export LANGCHAIN_MODEL=qwen2.5:14b

# æ ‡å‡†æ€§èƒ½ï¼ˆéœ€è¦ 8GB+ å†…å­˜ï¼‰
export LANGCHAIN_MODEL=qwen2.5:7b

# ä½Žé…ç½®ï¼ˆéœ€è¦ 4GB+ å†…å­˜ï¼‰
export LANGCHAIN_MODEL=qwen2.5:3b
```

### å¯¹è¯å¼é—®ç­”ï¼ˆæŽ¨è ChatGLM3ï¼‰
```bash
export LANGCHAIN_MODEL=chatglm3:6b
```

### è¶‹åŠ¿é¢„æµ‹ï¼ˆæŽ¨è DeepSeek-V2ï¼‰
```bash
export LANGCHAIN_MODEL=deepseek-v2:16b
export LANGCHAIN_TEMPERATURE=0.5  # æ›´é«˜åˆ›é€ æ€§
```

## 6. æ€§èƒ½ä¼˜åŒ–

### GPU åŠ é€Ÿ
Ollama è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ GPUï¼š
- NVIDIA GPUï¼šè‡ªåŠ¨ä½¿ç”¨ CUDA
- Apple Siliconï¼šè‡ªåŠ¨ä½¿ç”¨ Metal
- AMD GPUï¼šè‡ªåŠ¨ä½¿ç”¨ ROCm

### å¹¶å‘é…ç½®
```bash
# Ollama é…ç½®æ–‡ä»¶ (~/.ollama/config.json)
{
  "num_parallel": 4,  # å¹¶å‘è¯·æ±‚æ•°
  "num_ctx": 4096     # ä¸Šä¸‹æ–‡é•¿åº¦
}
```

## 7. å¸¸è§é—®é¢˜

### Q: Ollama éœ€è¦ç½‘ç»œå—ï¼Ÿ
A: åªæœ‰ä¸‹è½½æ¨¡åž‹æ—¶éœ€è¦ç½‘ç»œï¼Œè¿è¡Œæ—¶å®Œå…¨ç¦»çº¿ã€‚

### Q: å¦‚ä½•åˆ‡æ¢æ¨¡åž‹ï¼Ÿ
A: ä¿®æ”¹çŽ¯å¢ƒå˜é‡ `LANGCHAIN_MODEL` å³å¯ï¼Œæ— éœ€é‡å¯ Ollamaã€‚

### Q: å†…å­˜ä¸è¶³æ€Žä¹ˆåŠžï¼Ÿ
A: ä½¿ç”¨æ›´å°çš„æ¨¡åž‹ï¼Œå¦‚ `qwen2.5:7b` æˆ– `qwen2.5:3b`ã€‚

### Q: å¦‚ä½•æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡åž‹ï¼Ÿ
```bash
ollama list
```

### Q: å¦‚ä½•åˆ é™¤ä¸ç”¨çš„æ¨¡åž‹ï¼Ÿ
```bash
ollama rm qwen2.5:14b
```

### Q: Ollama æœåŠ¡æ²¡æœ‰å¯åŠ¨ï¼Ÿ
```bash
# å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# æˆ–åœ¨åŽå°è¿è¡Œ
ollama serve &
```

## 8. ä¸Ž OpenAI å¯¹æ¯”

| ç»´åº¦ | OpenAI (gpt-4o-mini) | Ollama (Qwen2.5-14B) |
|------|----------------------|----------------------|
| **ä¸­æ–‡èƒ½åŠ›** | â­â­â­â­â­ | â­â­â­â­â­ |
| **æˆæœ¬** | $0.15/1M tokens | **å…è´¹** |
| **é€Ÿåº¦** | å¿«ï¼ˆç½‘ç»œå»¶è¿Ÿï¼‰ | **å¾ˆå¿«ï¼ˆæœ¬åœ°ï¼‰** |
| **éšç§** | æ•°æ®ä¸Šä¼  | **å®Œå…¨æœ¬åœ°** |
| **ç¡¬ä»¶è¦æ±‚** | æ—  | 16GB+ RAM |
| **ç½‘ç»œä¾èµ–** | å¿…éœ€ | **ä»…ä¸‹è½½æ—¶** |
| **ç¨³å®šæ€§** | APIé™æµ | **æ— é™åˆ¶** |

## 9. æŽ¨èé…ç½®

### å¼€å‘çŽ¯å¢ƒï¼ˆæœ¬åœ°è°ƒè¯•ï¼‰
```bash
export LANGCHAIN_PROVIDER=ollama
export LANGCHAIN_MODEL=qwen2.5:7b
export LANGCHAIN_EMBEDDINGS_PROVIDER=ollama
export LANGCHAIN_EMBEDDINGS_MODEL=nomic-embed-text
```

### ç”Ÿäº§çŽ¯å¢ƒï¼ˆé«˜æ€§èƒ½ï¼‰
```bash
export LANGCHAIN_PROVIDER=ollama
export LANGCHAIN_MODEL=qwen2.5:14b
export LANGCHAIN_EMBEDDINGS_PROVIDER=ollama
export LANGCHAIN_EMBEDDINGS_MODEL=nomic-embed-text
export LANGCHAIN_MAX_TOKENS=4000
```

### ä½Žé…ç½®æœºå™¨
```bash
export LANGCHAIN_PROVIDER=ollama
export LANGCHAIN_MODEL=qwen2.5:3b
export LANGCHAIN_EMBEDDINGS_PROVIDER=ollama
export LANGCHAIN_EMBEDDINGS_MODEL=nomic-embed-text
```

## 10. è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰æ¨¡åž‹å‚æ•°
```bash
# åˆ›å»ºè‡ªå®šä¹‰ Modelfile
cat > Modelfile <<EOF
FROM qwen2.5:14b

# è®¾ç½®æ¸©åº¦
PARAMETER temperature 0.5

# è®¾ç½®ç³»ç»Ÿæç¤º
SYSTEM ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»åˆ†æžåŠ©æ‰‹
EOF

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡åž‹
ollama create my-qwen2.5 -f Modelfile

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡åž‹
export LANGCHAIN_MODEL=my-qwen2.5
```

### è¿œç¨‹ Ollama æœåŠ¡å™¨
```bash
# å¦‚æžœ Ollama è¿è¡Œåœ¨å…¶ä»–æœºå™¨ä¸Š
export LANGCHAIN_BASE_URL=http://192.168.1.100:11434
export LANGCHAIN_EMBEDDINGS_BASE_URL=http://192.168.1.100:11434
```

## 11. æ›´å¤šèµ„æº

- **Ollama å®˜ç½‘**: https://ollama.com
- **æ¨¡åž‹åº“**: https://ollama.com/library
- **Qwen2.5 ä»‹ç»**: https://huggingface.co/Qwen
- **TrendRadar æ–‡æ¡£**: ./README.md

---

**å¿«é€Ÿå¼€å§‹å‘½ä»¤**ï¼š
```bash
# 1. å®‰è£… Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. ä¸‹è½½æ¨¡åž‹
ollama pull qwen2.5:14b
ollama pull nomic-embed-text

# 3. é…ç½®çŽ¯å¢ƒ
export LANGCHAIN_PROVIDER=ollama
export LANGCHAIN_MODEL=qwen2.5:14b
export LANGCHAIN_EMBEDDINGS_PROVIDER=ollama

# 4. è¿è¡Œæµ‹è¯•
uv run python test_langchain_integration.py
```

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ðŸŽ‰
